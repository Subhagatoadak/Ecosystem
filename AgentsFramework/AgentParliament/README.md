Below is a sample README in Markdown format that explains the framework in detail:

---

# Dynamic Multi-Agent Problem Solving Framework

This framework is designed to tackle complex problems by leveraging a multi-agent architecture with LLM-based reasoning. It dynamically determines the required agent roles (or allows manual specification), instantiates agents, gathers reasoning via hierarchical discussion, aggregates votes and proposals, validates the solution, and produces a final, synthesized answer along with a full transparency report.

## Overview

The framework consists of several key components:

- **Dynamic Role Determination:**  
  An LLM call is used to determine the number and types of agents needed (government agents, domain experts, opposition agents, and subagents per government agent) based on the problem description. Users also have the option to manually specify these roles.

- **Agents:**  
  - **Government Agents (Ministers):** Responsible for proposing a solution. Each government agent gathers context from its subagents and generates a solution proposal using an LLM.
  - **Departmental Subagents:** Assist government agents by analyzing the problem and providing detailed insights.
  - **Domain Expert Agents:** Provide specialized, expert-level reasoning and proposals.
  - **Opposition Agents:** Critically evaluate the proposals, suggest improvements, and also generate their own solution proposals.
  - **Speaker/Moderator:** Oversees sessions, facilitates hierarchical discussions, and ensures transparency.

- **LLM-Based Reasoning:**  
  All solution proposals, reasoning, and votes are generated by making LLM calls via a helper function (`generate_llm_response`). This makes the system adaptable to a wide range of complex problems.

- **Voting Module:**  
  Agents cast their “votes” by providing their proposed solutions along with a brief rationale. The voting module aggregates these textual proposals and uses an LLM call to synthesize a final coherent solution along with the most compelling rationale.

- **Hierarchical Discussion & Transparency:**  
  A discussion tree is built from the contributions of all agents. The RTI (Right To Information) component logs the full discussion and decision process, making the framework fully transparent.

- **Validation:**  
  A validator function aggregates the chain-of-thought and reasoning, and uses an LLM to check that the final solution is consistent with the provided reasoning.

## File Structure

- **main.py:**  
  The main entry point that prompts the user for a problem, handles role specification (manual or dynamic), instantiates agents, runs the reasoning session, aggregates votes, and prints the final answer along with a transparency report.

- **llm_generator.py:**  
  A helper module (not shown here) that wraps LLM calls (e.g., to OpenAI's GPT models) via a function called `generate_llm_response`.

## Requirements

- **Python 3.7+**
- **OpenAI Python SDK:** Install via `pip install openai`
- **Environment Variable:** Set `OPENAI_API_KEY` with your API key.

## Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/your-repo.git
   cd your-repo
   ```
2. (Optional) Create a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```
3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
   *(Make sure `openai` is listed in your requirements.)*

4. Set your API key (for example, on Unix):
   ```bash
   export OPENAI_API_KEY="your_openai_api_key"
   ```

## Usage

Run the framework by executing:

```bash
python main.py
```

You will be prompted to enter a problem description. If you do not provide one, the default counterfeit coin puzzle will be used. Next, you can choose to manually specify the agent roles or let the system use an LLM to determine the roles dynamically.

For example, if you input a problem about determining which bag contains forged coins using a digital weighing machine, the system will simulate a digital weighing reading, run the reasoning session, aggregate agent proposals via voting, and finally produce a synthesized answer.

## Customization

- **LLM Prompts:**  
  All LLM calls are made via the `generate_llm_response` function. You can modify the prompt strings in each agent's methods (e.g., in `prepare_policy`, `vote`, `discuss`, etc.) to suit your domain-specific needs.

- **Dynamic Role Determination:**  
  The function `determine_roles(problem_text)` calls the LLM to produce a JSON specification of required roles. You can customize its prompt if you need different keys or role types.

- **Validation:**  
  The `validate_solution` function aggregates the chain-of-thought and passes it to the LLM for final consistency checking. Adjust the prompt here for more rigorous validation if necessary.

## Example Workflow

1. **Input Problem:**  
   User provides a complex problem description.
2. **Role Specification:**  
   The framework either uses manual input or calls the LLM to determine the agent roles.
3. **Agent Instantiation:**  
   Agents (government, domain experts, opposition, subagents, and moderator) are instantiated dynamically.
4. **Reasoning Session:**  
   Each agent generates its proposal using an LLM call.
5. **Voting:**  
   All agents vote on the proposed solutions. The LLM synthesizes a final solution from the aggregated proposals.
6. **Hierarchical Discussion:**  
   A discussion tree is created with contributions from all agents.
7. **Validation & Final Answer:**  
   The final solution is validated using the aggregated chain-of-thought and printed along with a transparency report.

## Final Output

At the end of the run, the framework prints:
- The **Final Synthesized Solution** (the final answer to the problem).
- The **Most Compelling Rationale** extracted from the agents’ votes.
- A **Validation Message** from the LLM assessing the consistency of the reasoning.
- An **LLM Generated Summary** of the hierarchical discussion.
- A **Full Transparency Report** showing all discussion and decision logs.
- Finally, a clearly labeled **FINAL ANSWER** section with the final solution.

## Conclusion

This dynamic multi-agent framework is built to be robust and adaptable for solving a wide variety of complex problems. By leveraging LLM-based reasoning at every step—from role determination to solution synthesis—it provides a transparent and traceable approach to collective decision-making. You can further customize prompts and logic to suit specific domains or problem types.

Feel free to fork and modify the code to fit your research or application needs!

---

This README provides comprehensive details about the code, its architecture, usage instructions, customization options, and an overview of the workflow.